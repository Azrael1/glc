{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "WARNING:tensorflow:From C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "# from PIL import Image\n",
    "import socket\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.special import softmax\n",
    "from tensorflow.keras import datasets, layers, models, activations\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from load_corrupted_data import CIFAR10, CIFAR100\n",
    "\n",
    "# note: nosgdr, schedule, and epochs are highly related settings\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Trains WideResNet on CIFAR',\n",
    "                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "# Optimization options\n",
    "parser.add_argument('--gold_fraction', '-gf', type=float, default=0.1, help='What fraction of the data should be trusted?')\n",
    "parser.add_argument('--corruption_prob', '-cprob', type=float, default=0.5, help='The label corruption probability.')\n",
    "parser.add_argument('--corruption_type', '-ctype', type=str, default='flip', help='Type of corruption (\"unif\" or \"flip\").')\n",
    "\n",
    "# random seed\n",
    "parser.add_argument('--seed', type=int, default=1)\n",
    "args = parser.parse_args(\"\")\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), transforms.ToTensor(),\n",
    "     transforms.Normalize(mean, std)])\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "gold_fraction = args.gold_fraction\n",
    "corruption_prob = args.corruption_prob\n",
    "corruption_type = args.corruption_type\n",
    "train_data_gold = CIFAR10(\n",
    "        'data', True, True, gold_fraction, corruption_prob, corruption_type,\n",
    "        transform=train_transform, download=True)\n",
    "train_data_silver = CIFAR10(\n",
    "        'data', True, False, gold_fraction, corruption_prob, corruption_type,\n",
    "        transform=train_transform, download=True, shuffle_indices=train_data_gold.shuffle_indices)\n",
    "train_data_gold_deterministic = CIFAR10(\n",
    "        'data', True, True, gold_fraction, corruption_prob, corruption_type,\n",
    "        transform=test_transform, download=True, shuffle_indices=train_data_gold.shuffle_indices)\n",
    "train_all_images = np.vstack((train_data_gold.train_data, train_data_silver.train_data))\n",
    "train_all_labels = np.array(train_data_gold.train_labels + train_data_silver.train_labels)\n",
    "train_all_labels_ = np.array([x -10 for x in train_data_gold.train_labels] + train_data_silver.train_labels)\n",
    "\n",
    "i = np.random.permutation(len(train_all_images))\n",
    "train_all_images, train_all_labels, train_all_labels_ = train_all_images[i], train_all_labels[i], train_all_labels_[i]\n",
    "\n",
    "\n",
    "test_data = CIFAR10('data', train=False, transform=test_transform, download=True)\n",
    "num_classes = 10\n",
    "\n",
    "def make_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3), kernel_initializer='random_normal', bias_initializer='zeros'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='random_normal', bias_initializer='zeros'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='random_normal', bias_initializer='zeros'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu', kernel_initializer='random_normal', bias_initializer='zeros'))\n",
    "    model.add(layers.Dense(10, kernel_initializer='random_normal', bias_initializer='zeros'))\n",
    "    model.add(layers.Activation(activations.linear))\n",
    "    return model\n",
    "\n",
    "# Train Silver model first \n",
    "\n",
    "def sparse_cat_entropy_loss(y_true, y_pred):\n",
    "    y_pred = K.softmax(y_pred)\n",
    "    y_true = K.cast(K.one_hot(K.cast(y_true[:, 0], tf.int32), num_classes), tf.float32)\n",
    "    \n",
    "    # scale predictions so that the class probas of each sample sum to 1\n",
    "    y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    loss = y_true * K.log(y_pred)# * weights\n",
    "    loss = -K.sum(loss, -1)\n",
    "    return loss\n",
    "\n",
    "base_model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "\n",
      " Silver model \n",
      "\n",
      "Train on 45000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/7\n",
      "45000/45000 [==============================] - 97s 2ms/sample - loss: 1.9914 - sparse_categorical_accuracy: 0.2564 - val_loss: 1.9078 - val_sparse_categorical_accuracy: 0.2558\n",
      "Epoch 2/7\n",
      "45000/45000 [==============================] - 95s 2ms/sample - loss: 1.7498 - sparse_categorical_accuracy: 0.3057 - val_loss: 1.9577 - val_sparse_categorical_accuracy: 0.2689\n",
      "Epoch 3/7\n",
      "45000/45000 [==============================] - 91s 2ms/sample - loss: 1.6445 - sparse_categorical_accuracy: 0.3326 - val_loss: 1.7747 - val_sparse_categorical_accuracy: 0.2724arse_\n",
      "Epoch 4/7\n",
      "45000/45000 [==============================] - 90s 2ms/sample - loss: 1.5743 - sparse_categorical_accuracy: 0.3542 - val_loss: 1.6858 - val_sparse_categorical_accuracy: 0.2675 3s - loss: 1.5729 - sparse_catego - ETA: 2s - loss: 1.5751 - sparse_categorical_accuracy: 0. - ETA: 1s - loss: 1.5748 - spar\n",
      "Epoch 5/7\n",
      "45000/45000 [==============================] - 89s 2ms/sample - loss: 1.5134 - sparse_categorical_accuracy: 0.3714 - val_loss: 1.7069 - val_sparse_categorical_accuracy: 0.3207\n",
      "Epoch 6/7\n",
      "45000/45000 [==============================] - 91s 2ms/sample - loss: 1.4582 - sparse_categorical_accuracy: 0.3888 - val_loss: 1.8384 - val_sparse_categorical_accuracy: 0.1982- sp\n",
      "Epoch 7/7\n",
      "45000/45000 [==============================] - 94s 2ms/sample - loss: 1.4093 - sparse_categorical_accuracy: 0.4032 - val_loss: 1.7105 - val_sparse_categorical_accuracy: 0.3172\n"
     ]
    }
   ],
   "source": [
    "model_s = tf.keras.models.clone_model(base_model)\n",
    "model_s.compile(optimizer='adam', loss=sparse_cat_entropy_loss,\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "print('\\n Silver model \\n')\n",
    "history_s = model_s.fit(train_data_silver.train_data, train_data_silver.train_labels, epochs=7,\n",
    "                    validation_data=(test_data.test_data, test_data.test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\BHPOHA\\Anaconda3\\envs\\bert\\lib\\site-packages\\scipy\\special\\_logsumexp.py:214: RuntimeWarning: invalid value encountered in subtract\n",
      "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
     ]
    }
   ],
   "source": [
    "# Form corruption matrix on silver model\n",
    "from scipy.special import softmax\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "def confidence_calibration(logits, y):\n",
    "       \"\"\"\n",
    "       Confidence calibration of the logits outside of tensorflow, based on the paper https://arxiv.org/pdf/1706.04599.pdf\n",
    "       Done using the method : Temperature Scaling \n",
    "       :param logits : Logits from neural network\n",
    "       :type logits : Pandas DataFrame / Numpy array\n",
    "       :param y : Multi/Single label Y\n",
    "       :type y : Pandas DataFrame\n",
    "       :return : Temperature Value, that will be used to scale the logits by\n",
    "       :rtype : Float \n",
    "       \"\"\"\n",
    "       logits = np.array(logits)\n",
    "       y = np.array(y.tolist()).astype('float32')\n",
    "       \n",
    "       def categorical_crossentropy_multi(y_true, y_pred):\n",
    "              # y_true, y_pred -> numberOfExamples, dimensionSize\n",
    "              losses = -1*np.log(np.maximum(np.sum(np.array(y_true) * np.array(y_pred), axis=1),1e-7)) # ignore '0'\n",
    "              loss = np.mean(losses)\n",
    "              return loss\n",
    "       def temperature_scaling(x):\n",
    "              \"\"\"\n",
    "              Temperature scaling for BERT.\n",
    "              logits : Logits : N, M : N = number of examples, M = number of classes\n",
    "              y : Target variable, N, M \n",
    "              x : Temperature \n",
    "              \"\"\"\n",
    "              scaled_logits = logits*1./x\n",
    "              return categorical_crossentropy_multi(y, softmax(scaled_logits, axis=-1))\n",
    "       res = minimize_scalar(temperature_scaling)\n",
    "       return res.x\n",
    "\n",
    "logits = model_s.predict(train_data_gold_deterministic.train_data)\n",
    "gold_labels = np.array([x -10 for x in train_data_gold_deterministic.train_labels])\n",
    "gold_one_hot = np.zeros((gold_labels.size, 10))\n",
    "gold_one_hot[np.arange(gold_labels.size), gold_labels] = 1\n",
    "T = confidence_calibration(logits, gold_one_hot)\n",
    "matrices = []\n",
    "\n",
    "logits_scaled = logits/T\n",
    "probs = softmax(logits_scaled, axis=-1)\n",
    "\n",
    "corruption_matrix = np.zeros((num_classes, num_classes))\n",
    "label_count = np.zeros(num_classes)\n",
    "for i, g_label in enumerate(train_data_gold_deterministic.train_labels):\n",
    "    corruption_matrix[g_label-10] += probs[i] # -10 because added +10 to differentiate from silver labels\n",
    "    label_count[g_label-10] += 1\n",
    "\n",
    "corruption_matrix = corruption_matrix/label_count[:, np.newaxis] \n",
    "matrices.append(corruption_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two different implementations of GLC\n",
    "\n",
    "### Version 1:\n",
    "\n",
    "- implements it by taking the corruption matrix, transposing it & indexing the matrix according to the labels generated\n",
    "- the resulting matrix is multiplied with the probabilties matrix to give the corrected probabilties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GLC model \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/7\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 3.1800 - sparse_categorical_accuracy: 0.2309 - val_loss: 3.2972 - val_sparse_categorical_accuracy: 0.3075\n",
      "Epoch 2/7\n",
      "50000/50000 [==============================] - 97s 2ms/sample - loss: 2.9330 - sparse_categorical_accuracy: 0.2796 - val_loss: 3.1622 - val_sparse_categorical_accuracy: 0.3584\n",
      "Epoch 3/7\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 2.8145 - sparse_categorical_accuracy: 0.3090 - val_loss: 3.1041 - val_sparse_categorical_accuracy: 0.3340\n",
      "Epoch 4/7\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 2.7442 - sparse_categorical_accuracy: 0.3244 - val_loss: 3.0589 - val_sparse_categorical_accuracy: 0.3735\n",
      "Epoch 5/7\n",
      "50000/50000 [==============================] - 99s 2ms/sample - loss: 2.6887 - sparse_categorical_accuracy: 0.3389 - val_loss: 2.9775 - val_sparse_categorical_accuracy: 0.4281\n",
      "Epoch 6/7\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 2.6340 - sparse_categorical_accuracy: 0.3533 - val_loss: 3.0765 - val_sparse_categorical_accuracy: 0.4071\n",
      "Epoch 7/7\n",
      "50000/50000 [==============================] - 94s 2ms/sample - loss: 2.5926 - sparse_categorical_accuracy: 0.3679 - val_loss: 3.0043 - val_sparse_categorical_accuracy: 0.3939 - lo\n"
     ]
    }
   ],
   "source": [
    "# Define GLC loss\n",
    "for i, corruption_matrix in enumerate(matrices):\n",
    "    def glc_loss(y_true, y_pred):\n",
    "        y_pred = tf.nn.softmax(y_pred)\n",
    "        silver_gold_ids = y_true[:, 0]>=10\n",
    "        silver_gold_ids = tf.cast(silver_gold_ids, tf.int32)\n",
    "        y_true_ = tf.where(tf.cast(silver_gold_ids, tf.bool), y_true[:, 0]-10, y_true[:, 0])\n",
    "        corruption_matrix_ = tf.gather(tf.cast(corruption_matrix.T, tf.float32), tf.cast(y_true_, tf.int32))\n",
    "\n",
    "        y_true = tf.cast(tf.one_hot(tf.cast(y_true_, tf.int32), num_classes), tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1-K.epsilon())\n",
    "        probabilities = y_pred\n",
    "        one_hot_labels = y_true\n",
    "\n",
    "        probabilities_corrected = probabilities * corruption_matrix_\n",
    "#         probabilities_corrected = probabilities_corrected/tf.math.reduce_sum(probabilities_corrected, axis=1, keepdims=True) \n",
    "        probabilities_corrected = tf.clip_by_value(probabilities_corrected, K.epsilon(), 1-K.epsilon())\n",
    "\n",
    "        silver_gold_ids_ = tf.expand_dims(silver_gold_ids, 1)\n",
    "        mix_loss_s = -tf.reduce_sum(one_hot_labels * tf.cast(tf.math.logical_not(tf.cast(silver_gold_ids_, tf.bool)), tf.float32) * tf.log(probabilities_corrected), axis=-1)\n",
    "        mix_loss_g = -tf.reduce_sum(one_hot_labels * tf.cast(silver_gold_ids_, tf.float32) * tf.log(probabilities), axis=-1)\n",
    "        mix_loss = mix_loss_s + mix_loss_g\n",
    "        per_example_loss =  mix_loss\n",
    "        return per_example_loss\n",
    "\n",
    "    model_glc = tf.keras.models.clone_model(base_model)\n",
    "    model_glc.compile(optimizer='adam', loss=glc_loss,\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    # Define GLC model\n",
    "\n",
    "    print('\\n GLC model \\n')\n",
    "    history_glc = model_glc.fit(train_all_images, train_all_labels, epochs=7,\n",
    "                        validation_data=(test_data.test_data, test_data.test_labels))\n",
    "    \n",
    "    # 60.39, came with really concentrated matrices, not spread out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29, 0.03, 0.09, 0.02, 0.06, 0.01, 0.35, 0.02, 0.07, 0.05],\n",
       "       [0.04, 0.3 , 0.03, 0.01, 0.12, 0.01, 0.04, 0.01, 0.03, 0.42],\n",
       "       [0.15, 0.05, 0.18, 0.05, 0.08, 0.06, 0.28, 0.04, 0.07, 0.03],\n",
       "       [0.17, 0.1 , 0.07, 0.14, 0.08, 0.12, 0.15, 0.05, 0.07, 0.04],\n",
       "       [0.21, 0.06, 0.08, 0.05, 0.19, 0.06, 0.18, 0.07, 0.06, 0.03],\n",
       "       [0.12, 0.17, 0.05, 0.1 , 0.05, 0.24, 0.14, 0.06, 0.04, 0.03],\n",
       "       [0.09, 0.05, 0.08, 0.04, 0.05, 0.04, 0.32, 0.02, 0.28, 0.04],\n",
       "       [0.1 , 0.05, 0.04, 0.04, 0.08, 0.06, 0.35, 0.24, 0.02, 0.03],\n",
       "       [0.09, 0.04, 0.3 , 0.02, 0.04, 0.01, 0.11, 0.01, 0.32, 0.06],\n",
       "       [0.06, 0.09, 0.04, 0.02, 0.31, 0.01, 0.07, 0.02, 0.03, 0.36]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(matrices[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Version 2\n",
    "\n",
    "Implements by \n",
    "\n",
    "- Doing a dot product of the corruption matrix and probabilities to give the corrected probabilties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " GLC model \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/7\n",
      "50000/50000 [==============================] - 101s 2ms/sample - loss: 2.6522 - sparse_categorical_accuracy: 0.1231 - val_loss: 2.0518 - val_sparse_categorical_accuracy: 0.3811\n",
      "Epoch 2/7\n",
      "50000/50000 [==============================] - 105s 2ms/sample - loss: 1.8709 - sparse_categorical_accuracy: 0.2211 - val_loss: 1.9831 - val_sparse_categorical_accuracy: 0.4634\n",
      "Epoch 3/7\n",
      "50000/50000 [==============================] - 101s 2ms/sample - loss: 1.7803 - sparse_categorical_accuracy: 0.2494 - val_loss: 1.9254 - val_sparse_categorical_accuracy: 0.5167\n",
      "Epoch 4/7\n",
      "50000/50000 [==============================] - 109s 2ms/sample - loss: 1.7231 - sparse_categorical_accuracy: 0.2669 - val_loss: 1.9022 - val_sparse_categorical_accuracy: 0.5274\n",
      "Epoch 5/7\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.6763 - sparse_categorical_accuracy: 0.2768 - val_loss: 1.9422 - val_sparse_categorical_accuracy: 0.5258\n",
      "Epoch 6/7\n",
      "50000/50000 [==============================] - 98s 2ms/sample - loss: 1.6477 - sparse_categorical_accuracy: 0.2818 - val_loss: 1.8607 - val_sparse_categorical_accuracy: 0.5584\n",
      "Epoch 7/7\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 1.6172 - sparse_categorical_accuracy: 0.2917 - val_loss: 1.8667 - val_sparse_categorical_accuracy: 0.5587ccu - ETA: 1s - loss: 1.6174 - sp\n"
     ]
    }
   ],
   "source": [
    "# Define GLC loss, scenario 1 \n",
    "for i, corruption_matrix in enumerate(matrices):\n",
    "    def glc_loss_(y_true, y_pred):\n",
    "        y_pred = tf.nn.softmax(y_pred)\n",
    "        silver_gold_ids = y_true[:, 0]>=10\n",
    "        silver_gold_ids = tf.cast(silver_gold_ids, tf.int32)\n",
    "        y_true_ = tf.where(tf.cast(silver_gold_ids, tf.bool), y_true[:, 0]-10, y_true[:, 0])\n",
    "\n",
    "        y_true = tf.cast(tf.one_hot(tf.cast(y_true_, tf.int32), num_classes), tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1-K.epsilon())\n",
    "        probabilities = y_pred\n",
    "        one_hot_labels = y_true\n",
    "        \n",
    "        probabilities_corrected = tf.linalg.matmul(probabilities, tf.cast(corruption_matrix, tf.float32))\n",
    "\n",
    "        silver_gold_ids_ = tf.expand_dims(silver_gold_ids, 1)\n",
    "        mix_loss_s = -tf.reduce_sum(one_hot_labels * tf.cast(tf.math.logical_not(tf.cast(silver_gold_ids_, tf.bool)), tf.float32) * tf.log(probabilities_corrected), axis=-1)\n",
    "        mix_loss_g = -tf.reduce_sum(one_hot_labels * tf.cast(silver_gold_ids_, tf.float32) * tf.log(probabilities), axis=-1)\n",
    "        mix_loss = mix_loss_s + mix_loss_g\n",
    "        per_example_loss =  mix_loss\n",
    "        return per_example_loss\n",
    "\n",
    "    model_glc_ = tf.keras.models.clone_model(base_model)\n",
    "    model_glc_.compile(optimizer='adam', loss=glc_loss_,\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "    # Define GLC model\n",
    "\n",
    "    print('\\n GLC model \\n')\n",
    "    history_glc_ = model_glc_.fit(train_all_images, train_all_labels, epochs=7,\n",
    "                        validation_data=(test_data.test_data, test_data.test_labels))\n",
    "    \n",
    "    # 60.39, came with really concentrated matrices, not spread out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Non-GLC model \n",
      "\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/7\n",
      "50000/50000 [==============================] - 96s 2ms/sample - loss: 2.0077 - sparse_categorical_accuracy: 0.2560 - val_loss: 1.9109 - val_sparse_categorical_accuracy: 0.3391\n",
      "Epoch 2/7\n",
      "50000/50000 [==============================] - 102s 2ms/sample - loss: 1.7347 - sparse_categorical_accuracy: 0.3140 - val_loss: 1.8019 - val_sparse_categorical_accuracy: 0.30751.7357 - sparse_categoric\n",
      "Epoch 3/7\n",
      "50000/50000 [==============================] - 102s 2ms/sample - loss: 1.6273 - sparse_categorical_accuracy: 0.3415 - val_loss: 1.6877 - val_sparse_categorical_accuracy: 0.4339\n",
      "Epoch 4/7\n",
      "50000/50000 [==============================] - 104s 2ms/sample - loss: 1.5548 - sparse_categorical_accuracy: 0.3660 - val_loss: 1.6146 - val_sparse_categorical_accuracy: 0.3974\n",
      "Epoch 5/7\n",
      "50000/50000 [==============================] - 103s 2ms/sample - loss: 1.4862 - sparse_categorical_accuracy: 0.3875 - val_loss: 1.6331 - val_sparse_categorical_accuracy: 0.3926\n",
      "Epoch 6/7\n",
      "50000/50000 [==============================] - 102s 2ms/sample - loss: 1.4377 - sparse_categorical_accuracy: 0.4028 - val_loss: 1.6160 - val_sparse_categorical_accuracy: 0.4110\n",
      "Epoch 7/7\n",
      "50000/50000 [==============================] - 101s 2ms/sample - loss: 1.3949 - sparse_categorical_accuracy: 0.4182 - val_loss: 1.6772 - val_sparse_categorical_accuracy: 0.4114\n"
     ]
    }
   ],
   "source": [
    "# Compare against non-GLC model\n",
    "\n",
    "print('\\n Non-GLC model \\n')\n",
    "\n",
    "model_n = tf.keras.models.clone_model(base_model)\n",
    "model_n.compile(optimizer='adam', loss=sparse_cat_entropy_loss,\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "history_n = model_n.fit(train_all_images, train_all_labels_, epochs=7,\n",
    "                    validation_data=(test_data.test_data, test_data.test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 509us/sample - loss: 1.6772 - sparse_categorical_accuracy: 0.4114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6771817258834838, 0.4114]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_model_acc = model_n.evaluate(test_data.test_data, test_data.test_labels)\n",
    "normal_model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 498us/sample - loss: 3.0043 - sparse_categorical_accuracy: 0.3939\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.004318510437012, 0.3939]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glc_model_acc = model_glc.evaluate(test_data.test_data, test_data.test_labels)\n",
    "glc_model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 503us/sample - loss: 1.8667 - sparse_categorical_accuracy: 0.5587s - loss: 1.8696 - sparse_categorical_accura\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8666670642852783, 0.5587]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glc_model_acc_ = model_glc_.evaluate(test_data.test_data, test_data.test_labels)\n",
    "glc_model_acc_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For values:\n",
    "\n",
    "- Gold Fraction : 0.1\n",
    "- Corruption Probability : 0.5\n",
    "- Corruption Type : flip \n",
    "\n",
    "#### Accuracies:\n",
    "\n",
    "- GLC model : 56%\n",
    "- Normal model : 41%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# proof of working\n",
    "y_true = tf.constant([[11], [0]])\n",
    "corruption_matrix = tf.constant([[0.6, 0.4], [0.7, 0.3]])\n",
    "y_pred = tf.constant([[0.9, 0.1], [0.8, 0.2]])\n",
    "silver_gold_ids = y_true[:, 0]>=10\n",
    "silver_gold_ids = tf.cast(silver_gold_ids, tf.int32)\n",
    "y_true_ = tf.where(tf.cast(silver_gold_ids, tf.bool), y_true[:, 0]-10, y_true[:, 0])\n",
    "                              \n",
    "y_true = tf.cast(tf.one_hot(tf.cast(y_true_, tf.int32), 2), tf.float32)\n",
    "y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1-K.epsilon())\n",
    "probabilities = y_pred\n",
    "one_hot_labels = y_true\n",
    "\n",
    "probabilities_corrected = tf.linalg.matmul(probabilities, tf.cast(corruption_matrix, tf.float32))\n",
    "probabilities_corrected = tf.clip_by_value(probabilities_corrected, K.epsilon(), 1-K.epsilon())\n",
    "\n",
    "silver_gold_ids_ = tf.expand_dims(silver_gold_ids, 1)\n",
    "mix_loss_s = -tf.reduce_sum(one_hot_labels * tf.cast(tf.math.logical_not(tf.cast(silver_gold_ids_, tf.bool)), tf.float32) * tf.log(probabilities_corrected), axis=-1)\n",
    "mix_loss_g = -tf.reduce_sum(one_hot_labels * tf.cast(silver_gold_ids_, tf.float32) * tf.log(probabilities), axis=-1)\n",
    "mix_loss = mix_loss_s + mix_loss_g\n",
    "per_example_loss =  mix_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.3025851, 0.4780358], dtype=float32),\n",
       " array([[0.9, 0.1],\n",
       "        [0.8, 0.2]], dtype=float32),\n",
       " array([[0.61      , 0.39      ],\n",
       "        [0.62      , 0.38000003]], dtype=float32),\n",
       " array([-0.       ,  0.4780358], dtype=float32),\n",
       " array([ 2.3025851, -0.       ], dtype=float32),\n",
       " array([[0., 1.],\n",
       "        [1., 0.]], dtype=float32),\n",
       " array([[-0.10536055, -2.3025851 ],\n",
       "        [-0.22314353, -1.609438  ]], dtype=float32),\n",
       " array([1, 0]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(per_example_loss), sess.run(probabilities), sess.run(probabilities_corrected), sess.run(mix_loss_s), sess.run(mix_loss_g), sess.run(one_hot_labels), sess.run(tf.log(probabilities)), sess.run(silver_gold_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.3025851], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.log(tf.constant([0.1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.3025851, 0.4780358], dtype=float32),\n",
       " array([[0.9, 0.1],\n",
       "        [0.8, 0.2]], dtype=float32),\n",
       " array([[0.61      , 0.39      ],\n",
       "        [0.62      , 0.38000003]], dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(per_example_loss), sess.run(probabilities), sess.run(probabilities_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def uniform_mix_C(mixing_ratio, num_classes):\n",
    "    '''\n",
    "    returns a linear interpolation of a uniform matrix and an identity matrix\n",
    "    '''\n",
    "    return mixing_ratio * np.full((num_classes, num_classes), 1 / num_classes) + \\\n",
    "        (1 - mixing_ratio) * np.eye(num_classes)\n",
    "\n",
    "def flip_labels_C(corruption_prob, num_classes, seed=1):\n",
    "    '''\n",
    "    returns a matrix with (1 - corruption_prob) on the diagonals, and corruption_prob\n",
    "    concentrated in only one other entry for each row\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    C = np.eye(num_classes) * (1 - corruption_prob)\n",
    "    row_indices = np.arange(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        C[i][np.random.choice(row_indices[row_indices != i])] = corruption_prob\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04]\n",
      " [0.04 0.64 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04]\n",
      " [0.04 0.04 0.64 0.04 0.04 0.04 0.04 0.04 0.04 0.04]\n",
      " [0.04 0.04 0.04 0.64 0.04 0.04 0.04 0.04 0.04 0.04]\n",
      " [0.04 0.04 0.04 0.04 0.64 0.04 0.04 0.04 0.04 0.04]\n",
      " [0.04 0.04 0.04 0.04 0.04 0.64 0.04 0.04 0.04 0.04]\n",
      " [0.04 0.04 0.04 0.04 0.04 0.04 0.64 0.04 0.04 0.04]\n",
      " [0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.64 0.04 0.04]\n",
      " [0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.64 0.04]\n",
      " [0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.64]]\n"
     ]
    }
   ],
   "source": [
    "print(uniform_mix_C(0.4, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6 0.  0.  0.  0.  0.  0.4 0.  0.  0. ]\n",
      " [0.  0.6 0.  0.  0.  0.  0.  0.  0.  0.4]\n",
      " [0.  0.  0.6 0.  0.  0.  0.4 0.  0.  0. ]\n",
      " [0.4 0.  0.  0.6 0.  0.  0.  0.  0.  0. ]\n",
      " [0.4 0.  0.  0.  0.6 0.  0.  0.  0.  0. ]\n",
      " [0.  0.4 0.  0.  0.  0.6 0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.6 0.  0.4 0. ]\n",
      " [0.  0.  0.  0.  0.  0.  0.4 0.6 0.  0. ]\n",
      " [0.  0.  0.4 0.  0.  0.  0.  0.  0.6 0. ]\n",
      " [0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.6]]\n"
     ]
    }
   ],
   "source": [
    "print(flip_labels_C(0.4, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
